{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d015da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Code Snippet Retrieval\n",
    "\n",
    "* Shiva Sai Pavan INJA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a4671",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c72ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Information Retrieval Problem:\n",
    "## Code Search and Snippet Retrieval\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "* **Description**:\n",
    "    * Develop a code search engine that allows users to search for and retrieve relevant code snippets and programming resources based on their queries.\n",
    "    * The results can include code snippets, libraries, and solutions to programming problems.\n",
    "    * We can present results by code similarity and by using ranking algorithms.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "* **Objective**: \n",
    "    * To develop a code search engine that can take both free-form user queries and code snippets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59005bae",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80c0aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Importance**:\n",
    "    1. *Code Quality Improvement*: By exploring various code snippets and solutions, developers can discover better coding practices, leading to improved code quality and maintainability.\n",
    "    \n",
    "    2. *Enhanced Productivity*: A code search engine significantly accelerates the software development process. Developers can quickly locate and reuse existing code snippets, libraries, and solutions, reducing redundant work and saving valuable time.\n",
    "    \n",
    "    3. *Bug Resolution*: When encountering bugs or issues in their code, developers can search for similar problems and solutions, which can help them troubleshoot and resolve issues more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93633183",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54caee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Summary of changes and updates since the last assignment submission**:\n",
    "    * <span style=\"color:green\">Added new ideas from the literature.</span>\n",
    "    * <span style=\"color:green\">Added 5 new literature papers from journals and conferences.</span>\n",
    "    * <span style=\"color:green\">Updated the chapter on hardware software and data.</span>\n",
    "    * <span style=\"color:green\">Updated the Tasks Accomplished, and Work Planned.</span>\n",
    "    * <span style=\"color:green\">Wrote the software code along with test cases.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fb267",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b8744",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Overview of Past and Current Solution Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e108f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## State-of-the-Art Solutions\n",
    "\n",
    "### New ideas from the literature:\n",
    "* Neural Code Search (NCS + TF-IDF + Cosine Similarity)\n",
    "* GloVe embedding model\n",
    "* Word2vec model\n",
    "* Doc2vec model\n",
    "\n",
    "### From the literature:\n",
    "* Aroma\n",
    "* FaCoY: **F**ind **a** **C**ode **o**ther than **Y**ours\n",
    "* CoCaBu: **CO**de vo**CABU**lary\n",
    "* GitSearch\n",
    "* DGMS: **D**eep **G**raph **M**atching and **S**earching\n",
    "* COSAL: **Co**de-to-Code **S**earch **A**cross **L**anguages\n",
    "* Search4Code\n",
    "\n",
    "_We will explain these tools in detail as we discuss the literature..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02056f07",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29133f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Available online:\n",
    "\n",
    "* Google Code Search\n",
    "\n",
    "* TranSˆ3\n",
    "\n",
    "* SLAMPA\n",
    "\n",
    "_In the interest of time and space, please see the \"Other Material\" in the \"Appendix\" for details..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f0b6d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cfe82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Solution Ideas from Journal and Conference Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea2aa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b805e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "* **Aroma `[Luan:2019:OOPSLA]`**:\n",
    "    * Takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet.\n",
    "    * Real time use; it first retrieves a small set of snippets based on approximate search, and then performs the pruning and clustering operations on this set. \n",
    "    * Recommend code snippets on a given query from a large corpus containing millions of methods on a multi-core server machine.\n",
    "    * Implemented in *C++* for four programming languages: \n",
    "        * *Hack*, *Java*, *JavaScript*, *Python*.\n",
    "        <!-- * [Julien Verlaguet and Alok Menghrajani. 2014. Hack: a new programming language for HHVM. https://code.fb.com/developer-tools/hack-a-new-programming-language-for-hhvm],  -->\n",
    "\n",
    "<center><img src=\"Figures/aroma_overview.png\" width=\"800\"/></center>\n",
    "<!-- ![aroma_overview.png](attachment:da5fb8ba-ecf0-4d30-8f82-4a76ee664615.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a21d48",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159f865",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **FaCoY `[Kim:2018:ICSE]`**:\n",
    "    * Code-to-code search tools; statically finding code fragments which may be semantically similar to user input code.\n",
    "    * Fully static; relies solely on source code with no constraint of having runtime information. \n",
    "    * Based on query alternation: after extracting structural code elements from a code fragment to build a query, build alternate queries using code fragments that present similar descriptions to the initial code fragment. \n",
    "    * Implemented indices for *Java* files collected from *GitHub* and Q&A posts from *StackOverflow*.\n",
    "\n",
    "<center><img src=\"Figures/facoy_overview.png\" width=\"800\"/></center>\n",
    "<!-- ![facoy_overview.png](attachment:cc95e454-c9db-47a7-8a4a-ba171cabb381.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f79eb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17206a38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **CoCaBu `[Sirres:2018:ICSE]`**: \n",
    "<!-- Keyword-based code search; built GITSEARCH, a code search engine, on top of GitHub and Stack Overflow. -->\n",
    "    * ``CoCaBu`` for the vocabulary mismatch problem: Finds relevant code with free-form query terms that describe the task, with no knowledge on the keywords/API to search. \n",
    "    * ``CoCaBu``’s Code Query Generator then creates another query which includes not only the initial user query terms but also program elements, such as method and class names, from the extracted snippets.\n",
    "    * ``GitSearch`` free-form search engine for GitHub: We instantiate the COCABU approach based on indices of Java files built from *GitHub* and Q&A posts from *StackOverflow* to find the most relevant source code examples for developer queries.\n",
    "    * ``GitSearch`` collects textual information in addition to structural entities. Treats source code as text, and conducts pre-processing such as *tokenization* (e.g., splitting camel case), *stop word removal*, and *stemming*.\n",
    "\n",
    "<center><img src=\"Figures/cocabu_overview.png\" width=\"800\"/></center>\n",
    "<!-- ![cocabu_overview.png](attachment:755f8571-9a44-40ae-833c-b7b136a3f680.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1b643",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a39dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **DeCo `[Cambronero:2019:ESEC/FSE]`**: \n",
    "    * This method embeds code into vectors and queries into vectors for semantic correlation, with supervised and unsupervised techniques.\n",
    "    * The query and the candidate code snippets are mapped to a shared vector space.\n",
    "    * Vector representations can be learned in an unsupervised manner, which just uses code, or in a supervised manner, which exploits an aligned corpus of code and their corresponding natural language descriptions.\n",
    "\n",
    "<center><img src=\"Figures/deeplearning.png\" width=\"1200\"/></center>\n",
    "<!-- ![deeplearning.png](attachment:8e1a3898-05f4-4e66-91cb-529ecca9358f.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d55819",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a6ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **COSAL `[Mathew:2021:ESEC/FSE]`**: \n",
    "  * Code snippets are ranked using non-dominated sorting based on code token similarity, structural similarity, and behavioral similarity.\n",
    "  * It uses two static similarity measures based on extracted tokens from source code, a tree edit distance based on a generic Abstract Syntax Tree (AST), and one dynamic similarity measure to compute IO similarity.\n",
    "\n",
    "<center><img src=\"Figures/COSAL.png\" width=\"1200\"/></center>\n",
    "<!-- ![deeplearning.png](attachment:8e1a3898-05f4-4e66-91cb-529ecca9358f.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7768b3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b358e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **DGMS `[Ling:2021:TKDD]`**: \n",
    "   * ``DGMS``is an end-to-end matching model based on graph neural networks for semantic code retrieval.\n",
    "   * ``DGMS`` makes better use of the rich structural information in source codes and query texts as well as the interaction semantic relations between each other.\n",
    "\n",
    "<center><img src=\"Figures/DGMS.png\" width=\"1200\"/></center>\n",
    "<!-- ![DGMS.png](attachment:003ea40f-3b20-4276-9c47-dcdfe3e3ff30.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cad423",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd2f13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Effective Reformulation of Query for Code Search using Crowdsourced Knowledge and Extra-Large Data Analytics** `[Rahman:2018:ICSME]`\n",
    "    * It is a technique that automatically identifies relevant and specific API classes from the StackOverflow Q&A site for a programming task written as a natural language query, then reformulates the query for improved code search.\n",
    "    * A total of 656538 Q&A threads are collected from StackOverflow, and then standard natural language preprocessing is performed on each thread to normalize the content.\n",
    "    * The baseline idea is to extract appropriate API classes from them using appropriate selection methods and use them for query reformulation.\n",
    "    * API Class Rank calculation and Borda score calculation are used to achieve the goals.\n",
    " \n",
    "<center><img src=\"Figures/NLP2API.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e1404",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbca89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Fast code recommendation via approximate sub-tree matching** `[Shao:2021:FITEE]`\n",
    "    * It takes programming context as input and recommends relevant code snippets to assist developers in software development.\n",
    "    * A new code recommendation algorithm based on sub-tree hashing and the SW algorithm is introduced.\n",
    "    * Experimental results show that it has good performance in terms of time consumption and accuracy for different recommending tasks.\n",
    "    * Code recommendation tool implemented for C and Java Language.\n",
    "    * The SW algorithm is used for sequence alignment to find similar fragments between the two sequences and determine their similarity.\n",
    "\n",
    "<center><img src=\"Figures/fast_code_recommendation.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938f2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef2a50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Search4Code: Code Search Intent Classification Using Weak Supervision** `[Rao:2021:MSR]`\n",
    "    * A novel weak supervision-based model to detect code search intent in queries, and this code recommendation works well at method and file level with good performance.\n",
    "    * A large-scale dataset of queries, mined from the Bing web search engine, used for code search research.\n",
    "    * A CNN-based model for code search intent classification has been developed for C# and Java search queries mined from the Bing web search engine.\n",
    "    * Implemented for the programming languages C# and Java..  \n",
    "    * This method requires additional storage to store the hash representation of AST nodes for code snippets, and the growth of this space  is  linearly related to the size of the code database.\n",
    " \n",
    "<center><img src=\"Figures/search4code.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cdd51",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d88e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Neural query expansion for code search** `[Liu:2019:MAPL]`\n",
    "    * The Neural Code Search (NCS) system facilitates code searching through natural language inquiries.\n",
    "    * It utilizes the proximity between vectorized representations of both code snippets and search terms to execute the search process.\n",
    "    * It incorporates a supervised learning component to align words in the search queries with terms found in the actual code.\n",
    "    * It employs a specialized ranking method to address the dispersal of search outcomes that often occurs with searches based on vector similarity.\n",
    "    * The capabilities of NCS are showcased through its ability to respond to programming queries sourced from Stack Overflow using a collection of code obtained from GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28a5a9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469ba91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Code search: A survey of techniques for finding code** `[Grazia:2023:ACMCS]`\n",
    "    * The paper reviews three decades of advancements in the field of code search research.\n",
    "    * Here are their key observations:\n",
    "        * The research indicates that while free-form queries are convenient to input and offer a broad range of expression, they might introduce ambiguity and lack the specificity of other structured query formats.\n",
    "        * Queries using programming languages don't require users to learn new syntax, yet the detail and expressiveness of such code-based queries fluctuate based on the user's search goal and the particularities of the search engine used.\n",
    "        * User interfaces have the potential to assist in the preprocessing of queries either transparently or through user feedback mechanisms.\n",
    "        * Code search engines typically refine queries by drawing on parallels between the search terms and code identifiers, as well as databases that pair natural language with code.\n",
    "        * Common methods for refining queries include adjusting the weight of search terms, adding new terms or substituting them, as well as enhancing queries with more sophisticated representations.\n",
    "        * Studies of developers reveal that code searches are routinely conducted to fulfill a variety of objectives, such as understanding existing code, locating reusable code segments, and efficiently navigating to familiar code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40e879",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc8594",
   "metadata": {},
   "source": [
    "* **GloVe `[Pennington:GloVe]`**:\n",
    "    * Glove is short for Global Vectors for Word Representation. It employs a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics.\n",
    "    * The model's architecture allows it to capture both local and global aspects of word semantics, by aggregating global co-occurrence statistics from a corpus. GloVe vectors have the advantage of being able to relate to the words’ co-occurrence probabilities, allowing for a rich representation of word meaning and nuanced differences.\n",
    "    * Effcient to read and use; we can use a pre-trained model to convert \"description\" and \"code\" from code datasets to meaningful embeddings for further quering.\n",
    "<center><img src=\"Figures/glove.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bfe6d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c7220",
   "metadata": {},
   "source": [
    "* **word2vec `[Pennington:word2vec]`**:\n",
    "    * Word2vec utilizes a shallow neural network architecture, either through a Continuous Bag of Words (CBOW) or Skip-gram model, focusing on word prediction either from context (CBOW) or predicting context from a word (Skip-gram).\n",
    "    * Enables the encoding of words into a vector space where semantic meaning is reflected in the distance and directionality between vectors. This allows for operations such as vector addition and subtraction to reveal semantic relationships between words, like solving analogies.\n",
    "    * It excels on tasks involving word analogy, word similarity, and named entity recognition, and can be trained on either aggregated global corpus statistics or on individual local context windows, providing flexibility and robustness in learning word embeddings.\n",
    "<center><img src=\"Figures/word2vec.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f9fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72520cc",
   "metadata": {},
   "source": [
    "* **doc2vec `[Le:word2vec]`**:\n",
    "    * Extends the word2vec paradigm by not only learning word embeddings but also learning document-level embeddings, capturing the essence of entire sentences, paragraphs, or documents in a continuous vector space.\n",
    "    * It Utilizes a \"Paragraph Vector\" framework which maintains a unique vector for each document that is trained to predict words in the document, enabling nuanced understanding of document semantics beyond individual word meanings.\n",
    "    * Facilitates a variety of document-level tasks such as document classification, sentiment analysis, and information retrieval by encoding the contextual information of words within a document, setting it apart from word-level embeddings.\n",
    "<center><img src=\"Figures/doc2vec.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ea074",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf94a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Solution Ideas Helpful\n",
    "* Treat source code as text, and conducts pre-processing such as tokenization (e.g., splitting camel case), stop word removal, and stemming.\n",
    "* Semantic vector idea: embed code into vectors like GloVe or word2vec.\n",
    "* Develop a comprehensive similarity determination method: based on token similarity, structural similarity, and behavioral similarity.\n",
    "* Graph neural networks on deep graph matching and searching (DGMS) model.\n",
    "\n",
    "<!-- > * Pick three best ideas from the literature afrter brainstorming\n",
    "> * detail which aspects of the papers we could use specificly \n",
    "> * Solution Idea 1\n",
    "> * Solution Idea 2 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a2a06",
   "metadata": {},
   "source": [
    "## Solution Ideas Helpful\n",
    "\n",
    "* **Pre-processing for Code Context**: Applies text pre-processing techniques to the code and its descriptions, such as tokenization (e.g., splitting identifiers and camelCase), removing less informative tokens (akin to stop words in natural language), and stemming to reduce words to their base or root form.\n",
    "* **Semantic Embedding of Code and Description**: Utilizes the GloVe model to embed code tokens into a vector space, taking advantage of the model's strength in capturing co-occurrence statistics within the corpus to reflect the semantic meaning of code components.\n",
    "* **Semantic Similarity for Search**: Establishes a method for computing similarity that encompasses semantic closeness between code descriptions, leveraging the dense vector representations from GloVe to match search queries with the most relevant code snippets based on their descriptions.\n",
    "* **Comprehensive Code Similarity Framework**: Constructs a similarity framework that accounts for the broader document-level context of code, using doc2vec embeddings to improve the matching of complex queries to relevant code documents, considering both structure and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8fe37",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9dc56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# New Solution Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bee673",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e920a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* Design a model that accepts free-form query terms and performs a static (on pre-processed data within our corpus) search. \n",
    "* Propose a semantic vector model based on codes AND comments. \n",
    "* Pre-processing code similarity with graph first and use novel match algorithms.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845b831",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4640f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Below are some key features of our solution:** *(we marked changes with colors and strikes)*\n",
    "1. ***Code Corpus:*** Collect and curate a vast repository of code snippets and programming resources from various sources, ~including public code repositories like GitHub, and Stack Overflow.~ <span style=\"color:green\">like GeeksForGeeks, W3Schools, TutorialsPoint, and ChatGPT.</span> Ensure that the codebase covers a wide range of ~programming languages and~ domains.\n",
    "2. ***Indexing and Parsing:*** Implement an indexing system to process and parse the collected codebase. Extract metadata, comments, and code structure to build an efficient search index. Tokenize the code to enable keyword-based searches.\n",
    "3. ***Query Interface:*** Develop a user-friendly query interface where users can input their programming queries. Allow for both text-based queries (e.g., \"Python JSON parsing\") and code-based queries (e.g., code snippet or function signature).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232df446",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10976fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "4. ***Search Algorithms:*** Implement search algorithms that can retrieve relevant code snippets and resources based on the user's query. Explore techniques like vector embeddings, semantic search, and natural language processing to improve search accuracy.\n",
    "5. ***Code Similarity:*** Calculate code similarity metrics to ensure that retrieved code snippets are not only relevant but also similar in functionality to what the user needs. Techniques such as code diffing and code embeddings can be useful here.\n",
    "6. ***Ranking and Relevance:*** Develop a ranking algorithm to sort search results by relevance. Take into account factors such as code quality, community feedback (e.g., upvotes on Stack Overflow), and recency of code updates.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "* **Optional**\n",
    "7. ***User Feedback:*** Allow users to provide feedback on the relevance and usefulness of retrieved code snippets. Use this feedback to continuously improve search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190a638",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72578f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<!--     8. *Code Visualization:* Provide a code visualization feature that allows users to see how the retrieved code snippets fit into larger code structures or projects. This can help users understand context. -->\n",
    "<!--     9. *Code Execution Sandbox:* Optionally, provide a sandbox environment where users can test and run code snippets within the search interface, enhancing the usability of the tool. -->\n",
    "\n",
    "\n",
    "* **Challenges and considerations:**\n",
    "    - *Data Privacy and Security:* Ensure that sensitive or proprietary code is not included in the code corpus. Implement security measures to protect user data.\n",
    "    - *Scalability:* Building and maintaining a large code corpus can be resource-intensive. Consider strategies for efficient storage and updates.\n",
    "    - *Code Quality:* Assess the quality of retrieved code snippets to avoid promoting poorly written or insecure code.\n",
    "    - *Legal Considerations:* Be mindful of copyright and licensing issues when collecting and distributing code snippets.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **Evaluation:** \n",
    "    * Evaluate the effectiveness of your code search engine using metrics such as **precision**, **recall**, and **user satisfaction** surveys. \n",
    "    * Conduct user testing to gather feedback and make improvements.\n",
    "    * Performance in terms of response time to queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec99c1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee1eca",
   "metadata": {},
   "source": [
    "# Hardware, Software, and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27902472",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3231b9",
   "metadata": {},
   "source": [
    "## Hardware, Software, and Data Needs\n",
    "\n",
    "We could use our laptops: \n",
    "* M1 Pro; 16 GB memory, 512 GB storage. MacOS 13.4.1.\n",
    "* 2.4 GHz 8-Core Intel Core i9 with Radeon Pro Vega 20 4 GB GPU; 32 GB memory, 2 TB storage. MacOS 14.1.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12da4f4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed65ffa",
   "metadata": {},
   "source": [
    "## Software System Diagram\n",
    "\n",
    "* Diagram of the flow of data and control for the software:\n",
    "\n",
    "<center><img src=\"Figures/IR_Project.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6e968",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ecc20",
   "metadata": {},
   "source": [
    "## Significant Software Tasks Accomplished\n",
    "\n",
    "* Curated first hundred entries for the dataset using websites like GeeksForGeeks, W3Schools, TutorialsPoint, and ChatGPT. We have manually ensured that the codebase covers a wide range of domains.</li>\n",
    "* Defined the JSON format for the data; containing description, code and tags fields. Below is the example for an entry in our dataset:</li>\n",
    "```\n",
    "    {\n",
    "      \"description\":\"Reverse a string in Python\",\n",
    "      \"code\":\"string[::-1]\",\n",
    "      \"tags\":[\n",
    "        \"python\",\n",
    "        \"string\",\n",
    "        \"reverse\"\n",
    "      ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7ccef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76c25c",
   "metadata": {},
   "source": [
    "* Implement an indexing system to process and parse the collected codebase. Extracted an index using our descriptions.\n",
    "* Preprocessed and tokenized the dataset and queries to enable free-form search. Below are some of the preprocessing ideas that we implemented:\n",
    "    * Case Standardization\n",
    "    * Tokenization\n",
    "    * Whitespace Removal\n",
    "    * Removal of non-ascii Characters\n",
    "    * Punctuation Removal\n",
    "    * Remove urls\n",
    "    * Stop-word Removal\n",
    "    * Common-word Removal\n",
    "    * Question-word Removal\n",
    "    * WordNetLemmatizer\n",
    "    * PorterStemmer\n",
    "    * SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0aefa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86bae2",
   "metadata": {},
   "source": [
    "* Wrote three search and ranking methods:\n",
    "    * The inititial method uses TF-IDF and cosine similarity scores.\n",
    "    * The second method uses Global Vectors for Word Representation (GloVe).\n",
    "        * GloVe is constructed by aggregating global word-word co-occurrence statistics from a corpus. \n",
    "        * The model effectively captures linear substructures in the vector space by leveraging both global statistics and local context, making it capable of providing insight into word analogies and semantic relationships. \n",
    "        * As a result, words with similar meanings are placed closer together in the vector space, reflecting their semantic similarity.\n",
    "    * The third method uses Doc2Vec model, also known as Paragraph Vector, is an extension of the Word2Vec approach to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs, and entire documents.\n",
    "        * Developed by researchers at Google, Doc2Vec represents documents in a high-dimensional vector space, \n",
    "        * This method not only considers the context of words within a document but also maintains a unique vector for the document itself. \n",
    "        * These document vectors are trained to predict words in a context, enabling the model to capture the semantic meaning of the text. As a result, semantically similar documents are mapped to proximate points in the vector space, facilitating tasks such as document similarity analysis, classification, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab0649",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822be345",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f4a22",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66151c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808acb5d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e6955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reference List\n",
    "\n",
    "* Reference 1: `[Luan:2019:OOPSLA]` Luan, Sifei, et al. \"Aroma: Code recommendation via structural code search.\" Proceedings of the ACM on Programming Languages 3.OOPSLA (2019): 1-28. https://dl.acm.org/doi/abs/10.1145/3360578\n",
    "\n",
    "* Reference 2: `[Kim:2018:ICSE]` Kim, Kisub, et al. \"FaCoY: a code-to-code search engine.\" Proceedings of the 40th International Conference on Software Engineering. 2018. https://dl.acm.org/doi/abs/10.1145/3180155.3180187\n",
    "\n",
    "* Reference 3: `[Sirres:2018:ICSE]` Mathew et al., Cross-language code search using static and dynamic analyses. In the proceedings of 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, 2021. https://dl.acm.org/doi/pdf/10.1145/3468264.3468538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a54aa8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053681b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* Reference 4: `[Cambronero:2019:ESEC/FSE]` Cambronero, Jose, et al. \"When deep learning met code search.\" Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2019. https://doi.org/10.1145/3338906.3340458\n",
    "\n",
    "* Reference 5: `[Mathew:2021:ESEC/FSE]` Mathew, George, and Kathryn T. Stolee. \"Cross-language code search using static and dynamic analyses.\" Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021. https://doi.org/10.1145/3468264.3468538\n",
    "\n",
    "* Reference 6: `[Ling:2021:TKDD]` Ling, Xiang, et al. \"Deep graph matching and searching for semantic code retrieval.\" ACM Transactions on Knowledge Discovery from Data (TKDD) 15.5 (2021): 1-21.\n",
    " https://doi.org/10.1145/3447571\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b22768",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94679ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* Reference 7: `[Rahman:2018:ICSME]` Rahman, Mohammad Masudur, and Chanchal Roy. \"Effective reformulation of query for code search using crowdsourced knowledge and extra-large data analytics.\" 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2018. https://doi.org/10.1109/ICSME.2018.00057\n",
    "\n",
    "* Reference 8: `[Shao:2021:FITEE]` Shao, Yichao, et al. \"Fast code recommendation via approximate sub-tree matching.\" Frontiers of Information Technology & Electronic Engineering 23.8 (2022): 1205-1216. https://doi.org/10.1631/FITEE.2100379\n",
    "\n",
    "* Reference 9: `[Rao:2021:MSR]` Rao, Nikitha, Chetan Bansal, and Joe Guan. \"Search4Code: Code search intent classification using weak supervision.\" 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021. https://doi.org/10.1109/MSR52588.2021.00077\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21bb8b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001707f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* Reference 10: `[Liu:2019:MAPL]` Liu, Jason, et al. \"Neural query expansion for code search.\" Proceedings of the 3rd acm sigplan international workshop on machine learning and programming languages. 2019. https://doi.org/10.1145/3315508.3329975\n",
    "\n",
    "* Reference 11: `[Grazia:2023:ACMCS]` Di Grazia, Luca, and Michael Pradel. \"Code search: A survey of techniques for finding code.\" ACM Computing Surveys 55.11 (2023): 1-31. https://doi.org/10.1145/3565971\n",
    "\n",
    "* Reference 12: `[Pennington:2014:EMNLP]` Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. \"Glove: Global vectors for word representation.\" Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.\n",
    "\n",
    "* Reference 13: `[Mikolov:2013:ANIPS]` Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems 26 (2013).\n",
    "\n",
    "* Reference 14: `[Le:2014:PMLR]` Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. PMLR, 2014.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e1e85",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebd466",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Other Material\n",
    "* Google Code Search: https://code.google.com.\n",
    "\n",
    "<!-- * Search4Code, Microsoft: https://github.com/microsoft/Search4Code. -->\n",
    "\n",
    "* Wenhua Wang, Yuqun Zhang, Zhengran Zeng, and Guandong Xu. 2020. TranSˆ3: A Transformer-based Framework for Unifying Code Summarization and Code Search. CoRR abs/2003.03238 (2020). arXiv:2003.03238 https://arxiv.org/abs/2003.03238.\n",
    "\n",
    "* S. Zhou, H. Zhong, and B. Shen. 2018. SLAMPA: Recommending Code Snippets with Statistical Language Model. In 2018 25th Asia-Pacific Software Engineering Conference (APSEC). 79–88.\n",
    "\n",
    "* GloVe project website: https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7368b91",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7752e",
   "metadata": {},
   "source": [
    "# Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466ca00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-G2A0SJV\n",
      "desktop-g2a0sjv\\i.shiva sai pavan\n",
      "C:\\Users\\I.Shiva Sai Pavan\\anaconda3\\python.exe\n",
      "3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# Cells for the well organized and documented code \n",
    "# --------------------------------------------------------------------------------\n",
    "# Preamble script block to identify host, user, and kernel\n",
    "import sys\n",
    "! hostname\n",
    "! whoami\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c9bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "import json \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import time\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2076ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Uncomment below if you do not have these on your system\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea940d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# BEWARE: Slow to load!\n",
    "# import gensim.downloader as api\n",
    "# --------------------------------------------------------------------------------\n",
    "# load glove vectors from twitter 2015\n",
    "# model_glove = api.load(\"glove-twitter-100\") \n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16953a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Read a pre-trained glove model\n",
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    model_glove = {}\n",
    "    with open(File,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            model_glove[word] = embedding\n",
    "    print(f\"{len(model_glove)} words loaded!\")\n",
    "    return model_glove\n",
    "\n",
    "model_glove = load_glove_model(\"Models/glove.6B.50d.txt\")\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3611cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# BEWARE: Slow to load!\n",
    "# --------------------------------------------------------------------------------\n",
    "# text8_dataset = api.load(\"text8\")  # load dataset as iterable (Cleaned small sample from wikipedia)\n",
    "# --------------------------------------------------------------------------------\n",
    "# model_w2v = Word2Vec(text8_dataset)  # train w2v model\n",
    "# model_w2v.wv.save_word2vec_format('test_w2v.txt', binary=False)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7783093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec Model\n",
      "71291 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Read a pre-trained word2vec model\n",
    "# model_w2v = KeyedVectors.load_word2vec_format(\"Models/test_w2v.txt\")\n",
    "# Read a pre-trained glove model\n",
    "def load_word2vec_model(File):\n",
    "    print(\"Loading Word2vec Model\")\n",
    "    model_w2v = {}\n",
    "    with open(File,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            model_w2v[word] = embedding\n",
    "    print(f\"{len(model_w2v)} words loaded!\")\n",
    "    return model_w2v\n",
    "\n",
    "model_w2v = load_word2vec_model(\"Models/test_w2v.txt\")\n",
    "# --------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c6d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "# pretty dictionary printer: https://stackoverflow.com/questions/3229419/how-to-pretty-print-nested-dictionaries\n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "def printResults(results, n):\n",
    "    if n > len(results):\n",
    "        print(\"ERROR: Number of results requested (\" + str(n) + \") is greater than available (\" + str(len(results)) + \") results!\")\n",
    "    else: \n",
    "        i = 0;\n",
    "        for r in results[0:n]:\n",
    "            print('Result #' + str(i+1) + \":\\n\")\n",
    "            pretty(r)\n",
    "            i = i + 1\n",
    "            print('--------------------------------------------------------------------------------\\n')\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845a1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(d) is : <class 'dict'>\n",
      "type(d['dataset']) is : <class 'list'>\n",
      "len(d['dataset']) is : 100\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Dataset / Code Corpus\n",
    "\n",
    "with open('Dataset/dataset.json') as f:\n",
    "    d = json.load(f)\n",
    "    # print(d)\n",
    "    # jdata = json.loads(f)\n",
    "    # for d in jdata:\n",
    "    #     for key, value in d.iteritems():\n",
    "    #         print(key, value)\n",
    "    \n",
    "print(\"type(d) is : \" + str(type(d)))\n",
    "print(\"type(d['dataset']) is : \" + str(type(d['dataset'])))\n",
    "print(\"len(d['dataset']) is : \" + str(len(d['dataset'])))\n",
    "\n",
    "dataset = d['dataset']\n",
    "    \n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dcfaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Preprocessing\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "question_words = {\"what\", \"who\", \"whom\", \"whose\", \"which\", \"where\", \"when\", \"why\", \"how\"}\n",
    "\n",
    "common_words = {\"please\", \"code\", \"program\", \"write\"}\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     return preprocess_text(text, 3, \"SNOWBALL\")\n",
    "\n",
    "def preprocess_text(text, removal, method):\n",
    "    text = text.lower()                                                 # Case standardization\n",
    "    \n",
    "    tokens = word_tokenize(text)                                        # Tokenize \n",
    "    \n",
    "    tokens = [token.strip() for token in tokens if token.strip()]       # Remove whitespace\n",
    "\n",
    "    tokens = [re.sub(r'[^\\x00-\\x7f]',r' ', token) for token in tokens]  # remove non-ascii (everything without a code from x00 to x7f)\n",
    "    \n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]        # Remove punctuation (everything not a word or space)\n",
    "\n",
    "    tokens = [re.sub(r'https', '', token) for token in tokens]          # Remove urls\n",
    "    \n",
    "    #Remove irrelevant text\n",
    "    if removal == 1:\n",
    "        tokens = [i for i in tokens if not i in stop_words]             # stop-word removal\n",
    "    elif removal == 2:\n",
    "        tokens = [i for i in tokens if not i in stop_words]             # stop-word removal\n",
    "        tokens = [i for i in tokens if not i in question_words]         # question-word removal\n",
    "    elif removal == 3:\n",
    "        tokens = [i for i in tokens if not i in stop_words]             # stop-word removal\n",
    "        tokens = [i for i in tokens if not i in question_words]         # question-word removal\n",
    "        tokens = [i for i in tokens if not i in common_words]           # common-word removal\n",
    "\n",
    "    #lemmatization: typically preferred over stemming\n",
    "    if method == 'LEMMER':\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        tokens = [lemmer.lemmatize(word) for word in tokens]\n",
    "    elif method == 'PORTER':\n",
    "        porter = PorterStemmer()\n",
    "        tokens = [porter.stem(token) for token in tokens]\n",
    "    elif method == 'SNOWBALL':\n",
    "        snowball = SnowballStemmer(\"english\")\n",
    "        tokens = [snowball.stem(token) for token in tokens]     \n",
    "        \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693aebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "{'been', 'she', \"that'll\", 'in', 'will', 'did', 're', 'out', 'was', 'than', \"you'll\", 'their', 'until', 'a', 'ma', 'ourselves', 'it', \"shouldn't\", 'from', 'these', 'again', 'my', \"isn't\", 'won', 'why', 'through', 'yourselves', 'down', \"should've\", 'had', 'by', 'hasn', 'wouldn', \"mustn't\", \"didn't\", 'most', 'me', 'which', 'with', 'about', 'before', 'has', 'themselves', \"she's\", 'him', 'shan', 'be', 'what', \"aren't\", \"won't\", 'and', 'can', 'the', 'further', 'there', 'very', 'hadn', 'ours', 'its', \"you've\", 'wasn', 'off', 'this', 'but', 'into', 'once', 'more', 'just', \"couldn't\", 'd', 'aren', 'i', 'against', 'shouldn', 'ain', 'of', \"it's\", 'for', 'you', \"you'd\", 'up', 'few', 'at', 'same', 'your', 'under', 'how', 'having', 'if', 'only', \"haven't\", \"hasn't\", \"you're\", 'didn', 'or', 'they', \"don't\", 'he', 'yours', 'whom', 'needn', 'himself', 'while', 'are', 'that', 'don', \"needn't\", 'her', 'where', 'above', 'herself', 'here', 's', 'itself', 'who', 'on', 'both', 'should', 'when', 'below', 'myself', 'all', 'couldn', 'doesn', 'not', 'his', 'll', 'were', 'have', 'so', 'our', 'an', \"wasn't\", 'other', \"shan't\", 'own', 'to', 'them', 'over', 'because', 'as', 'such', 'isn', 'some', 'after', 'theirs', 'then', 've', 'am', 'now', 'mustn', 'm', 'between', \"wouldn't\", 'any', 'each', \"mightn't\", 'nor', \"hadn't\", 'yourself', 'mightn', 'during', 'is', 'o', 'y', 'too', 'does', 'haven', 't', 'we', 'weren', \"doesn't\", 'being', 'those', 'no', \"weren't\", 'do', 'doing', 'hers'}\n",
      "--------------------------------------------------------------------------------\n",
      "{'why', 'what', 'where', 'which', 'how', 'whom', 'who', 'whose', 'when'}\n",
      "--------------------------------------------------------------------------------\n",
      "{'program', 'write', 'please', 'code'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(stop_words)\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(question_words)\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(common_words)\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4582ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split([entry['description'] for entry in dataset], \n",
    "#                                                     [entry['code'] for entry in dataset],\n",
    "#                                                     test_size=0.3,\n",
    "#                                                     random_state=0)\n",
    "# print(X_train.shape, X_test.shape)\n",
    "\n",
    "# print(\"Length of X_train:\\t\" + str(len(X_train)))\n",
    "\n",
    "# print(\"Length of X_test:\\t\" + str(len(X_test)))\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3728efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Search & Ranking:\n",
    "\n",
    "# Vector Representation: \n",
    "# The first method, we use TF-IDF.\n",
    "# --------------------------------------------------------------------------------\n",
    "# Combine descriptions and code to create a corpus\n",
    "corpus = [entry[\"description\"] + \" \" + entry[\"code\"] for entry in dataset]\n",
    "corpus_description = [entry[\"description\"] for entry in dataset]\n",
    "vectorizer = TfidfVectorizer().fit(corpus)\n",
    "# --------------------------------------------------------------------------------\n",
    "# def tfidf_search(query):\n",
    "#     return tfidf_search(query, 3, \"SNOWBALL\")\n",
    "# --------------------------------------------------------------------------------\n",
    "def tfidf_search(query, removal, method):\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_text(query, removal, method)\n",
    "\n",
    "    print(\"Searching for processed query of : \\n\\t\" + str(processed_query) + \"\\n\")\n",
    "    \n",
    "    # Convert the query to a vector\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = cosine_similarity(query_vector, vectorizer.transform(corpus))\n",
    "    \n",
    "    # Rank the results\n",
    "    ranked_indices = similarity_scores.argsort()[0][::-1]\n",
    "\n",
    "    print(\"Best match scored cosine similarity of \" + str(similarity_scores.max()) + \"; the worst match scored \" + str(similarity_scores.min()) + \".\\n\")\n",
    "    \n",
    "    return [dataset[i] for i in ranked_indices]\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e304145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Search & Ranking:\n",
    "\n",
    "# Vector Representation: \n",
    "# The second one: use GloVe model\n",
    "# GloVe, which stands for Global Vectors for Word Representation,\n",
    "# is an unsupervised learning model. \n",
    "# GloVe is constructed by aggregating global word-word co-occurrence statistics from a corpus. \n",
    "# The model effectively captures linear substructures in the vector space by leveraging both global statistics and local context, making it capable of providing insight into word analogies and semantic relationships. \n",
    "# As a result, words with similar meanings are placed closer together in the vector space, reflecting their semantic similarity.\n",
    "# --------------------------------------------------------------------------------\n",
    "def sentence_to_glove_vec(sentence, model):\n",
    "    # Lowercase the sentence\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    # Remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in model.keys()]\n",
    "    \n",
    "    # If no words in the model, return a zero vector\n",
    "    if not words:\n",
    "        return np.zeros(50)\n",
    "    \n",
    "    # Sum up the word vectors\n",
    "    sentence_vector = np.sum([model[word] for word in words], axis=0)\n",
    "    \n",
    "    # Take the mean of the vectors\n",
    "    return sentence_vector / len(words)\n",
    "# --------------------------------------------------------------------------------\n",
    "# def GloVe_search(query):\n",
    "#     return GloVe_search(query, 3, \"SNOWBALL\")\n",
    "# --------------------------------------------------------------------------------\n",
    "def GloVe_search(query, removal, method):\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_text(query, removal, method)\n",
    "\n",
    "    print(\"Searching for processed query of : \\n\\t\" + str(processed_query) + \"\\n\")\n",
    "    \n",
    "    # Convert the query to a vector\n",
    "    query_vector = sentence_to_glove_vec(processed_query, model_glove)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    sim_scores = []\n",
    "    for entry in dataset:\n",
    "        description = entry[\"description\"]\n",
    "        processed_des = preprocess_text(description, removal, method)\n",
    "        des_vector = sentence_to_glove_vec(processed_des, model_glove)\n",
    "        sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n",
    "        \n",
    "    # Rank the results\n",
    "    similarity_scores = np.array(sim_scores)\n",
    "    ranked_indices = similarity_scores.argsort()[::-1]\n",
    "    \n",
    "\n",
    "    print(\"Best match scored cosine similarity of \" + str(similarity_scores.max()) + \"; the worst match scored \" + str(similarity_scores.min()) + \".\\n\")\n",
    "    \n",
    "    return [dataset[i] for i in ranked_indices]\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec6e2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Vector Representation: \n",
    "# The third one: use word2vec model\n",
    "# Word2Vec is a neural network-based technique developed by Google to convert words into numerical form, \n",
    "# where semantically similar words are mapped to proximate points in a multidimensional space.\n",
    "# --------------------------------------------------------------------------------\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Lowercase the sentence\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    # Remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in model.keys()]\n",
    "    \n",
    "    # If no words in the model, return a zero vector\n",
    "    if not words:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    # Sum up the word vectors\n",
    "    sentence_vector = np.sum([model[word] for word in words], axis=0)\n",
    "    \n",
    "    # Take the mean of the vectors\n",
    "    return sentence_vector / len(words)\n",
    "# --------------------------------------------------------------------------------\n",
    "# def word2vec_search(query):\n",
    "#     return word2vec_search(query, 3, \"SNOWBALL\")\n",
    "# --------------------------------------------------------------------------------\n",
    "def word2vec_search(query, removal, method):\n",
    "    processed_query = preprocess_text(query, removal, method)\n",
    "\n",
    "    print(\"Searching for processed query of : \\n\\t\" + str(processed_query) + \"\\n\")\n",
    "    \n",
    "    # Convert the query to a vector\n",
    "    query_vector = sentence_to_vec(processed_query, model_w2v)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    sim_scores = []\n",
    "    for entry in dataset:\n",
    "        description = entry[\"description\"]\n",
    "        processed_des = preprocess_text(description, removal, method)\n",
    "        des_vector = sentence_to_vec(processed_des, model_w2v)\n",
    "        sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n",
    "    \n",
    "    # Rank the results\n",
    "\n",
    "    similarity_scores = np.array(sim_scores)\n",
    "    similarity_scores_without_nan = similarity_scores[~np.isnan(similarity_scores)]\n",
    "    similarity_scores_with_inf = np.nan_to_num(similarity_scores, nan=-np.inf)\n",
    "    # Get the indices that would sort the array, then slice to get the last five entries\n",
    "    sorted_indices = np.argsort(similarity_scores_with_inf)[-5:]\n",
    "    # Since we want the largest values, we reverse the sorted indices\n",
    "    ranked_indices = sorted_indices[::-1]\n",
    "\n",
    "    print(\"Best match scored cosine similarity of \" + str(similarity_scores_without_nan.max()) + \"; the worst match scored \" + str(similarity_scores_without_nan.min()) + \".\\n\")\n",
    "    \n",
    "    return [dataset[i] for i in ranked_indices]\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bac1cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Vector Representation: \n",
    "# The fourth one: use doc2vec model\n",
    "# # Doc2Vec, also known as Paragraph Vector, \n",
    "# is an extension of the Word2Vec approach to unsupervised learning of continuous representations for larger blocks of text, \n",
    "# such as sentences, paragraphs, and entire documents. \n",
    "# Developed by researchers at Google, Doc2Vec represents documents in a high-dimensional vector space, \n",
    "# This method not only considers the context of words within a document but also maintains a unique vector for the document itself. \n",
    "# These document vectors are trained to predict words in a context, enabling the model to capture the semantic meaning of the text. As a result, semantically similar documents are mapped to proximate points in the vector space, facilitating tasks such as document similarity analysis, classification, and clustering.\n",
    "# --------------------------------------------------------------------------------\n",
    "# NOT SUITABLE FOR THE LENGTH OF OUR DESCRIPTIONS\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9560f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for processed query of : \n",
      "\trevers string \n",
      "\n",
      "Best match scored cosine similarity of 0.8038446020918463; the worst match scored 0.0.\n",
      "\n",
      " --------------------------------------------- 0.015004158020019531 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tReverse a string in Python\n",
      "code\n",
      "\tstring[::-1]\n",
      "tags\n",
      "\t['python', 'string', 'reverse']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tConvert a list of strings into a single string\n",
      "code\n",
      "\tjoined_string = ' '.join(my_list_of_strings)\n",
      "tags\n",
      "\t['python', 'string', 'join', 'list']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tDetermine if a string has all unique characters.\n",
      "code\n",
      "\t\n",
      "def is_unique(s):\n",
      "    return len(set(s)) == len(s)\n",
      "\n",
      "# Example Usage:\n",
      "unique_string = \"abcdef\"\n",
      "not_unique_string = \"aabbcc\"\n",
      "is_unique(unique_string)  # Returns True\n",
      "is_unique(not_unique_string)  # Returns False\n",
      "        \n",
      "tags\n",
      "\t['string', 'unique characters']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\trevers string \n",
      "\n",
      "Best match scored cosine similarity of 0.8035751356169228; the worst match scored -0.1596426333593455.\n",
      "\n",
      " --------------------------------------------- 0.031900882720947266 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tDetermine if a string has all unique characters.\n",
      "code\n",
      "\t\n",
      "def is_unique(s):\n",
      "    return len(set(s)) == len(s)\n",
      "\n",
      "# Example Usage:\n",
      "unique_string = \"abcdef\"\n",
      "not_unique_string = \"aabbcc\"\n",
      "is_unique(unique_string)  # Returns True\n",
      "is_unique(not_unique_string)  # Returns False\n",
      "        \n",
      "tags\n",
      "\t['string', 'unique characters']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tReverse a string in Python\n",
      "code\n",
      "\tstring[::-1]\n",
      "tags\n",
      "\t['python', 'string', 'reverse']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tReverse a string using a stack\n",
      "code\n",
      "\t\n",
      "def reverse_string(s):\n",
      "    stack = list(s)\n",
      "    reversed_string = ''\n",
      "    while stack:\n",
      "        reversed_string += stack.pop()\n",
      "    return reversed_string\n",
      "\n",
      "reversed_s = reverse_string('hello')\n",
      "        \n",
      "tags\n",
      "\t['stack', 'string']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\trevers string \n",
      "\n",
      "Best match scored cosine similarity of 1.0; the worst match scored 0.033832824253751746.\n",
      "\n",
      " --------------------------------------------- 0.02997875213623047 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tDetermine if a string has all unique characters.\n",
      "code\n",
      "\t\n",
      "def is_unique(s):\n",
      "    return len(set(s)) == len(s)\n",
      "\n",
      "# Example Usage:\n",
      "unique_string = \"abcdef\"\n",
      "not_unique_string = \"aabbcc\"\n",
      "is_unique(unique_string)  # Returns True\n",
      "is_unique(not_unique_string)  # Returns False\n",
      "        \n",
      "tags\n",
      "\t['string', 'unique characters']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tReverse a string in Python\n",
      "code\n",
      "\tstring[::-1]\n",
      "tags\n",
      "\t['python', 'string', 'reverse']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tConvert a list of strings into a single string\n",
      "code\n",
      "\tjoined_string = ' '.join(my_list_of_strings)\n",
      "tags\n",
      "\t['python', 'string', 'join', 'list']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/f_llb0z97sjdnv60m892yls80000gn/T/ipykernel_5356/603531733.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Test Session\n",
    "start_time = time.time()\n",
    "results = tfidf_search(\"How can I reverse a string?\", 3, \"SNOWBALL\")\n",
    "print(\" --------------------------------------------- %s seconds ---\" % (time.time() - start_time))\n",
    "printResults(results, 3)\n",
    "print('\\n================================================================================\\n')\n",
    "start_time = time.time()\n",
    "results = GloVe_search(\"How can I reverse a string?\", 3, \"SNOWBALL\")\n",
    "print(\" --------------------------------------------- %s seconds ---\" % (time.time() - start_time))\n",
    "printResults(results, 3)\n",
    "print('\\n================================================================================\\n')\n",
    "start_time = time.time()\n",
    "results = word2vec_search(\"How can I reverse a string?\", 3, \"SNOWBALL\")\n",
    "print(\" --------------------------------------------- %s seconds ---\" % (time.time() - start_time))\n",
    "printResults(results, 3)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23268a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_utility(query, removal, method, top):\n",
    "    print(\"User given query:\\n\\t\" + str(query) + \"\")\n",
    "    \n",
    "    print('\\n\\n === TF-IDF SEARCH ==============================================================\\n')\n",
    "    start_time = time.time()\n",
    "    results = tfidf_search(query, removal, method)\n",
    "    print(\" ------------------------------------------------------------- %s seconds ---\" % round((time.time() - start_time), 4))\n",
    "    printResults(results, top)\n",
    "    print('\\n\\n === GloVe SEARCH ===============================================================\\n')\n",
    "    start_time = time.time()\n",
    "    results = GloVe_search(query, removal, method)\n",
    "    print(\" ------------------------------------------------------------- %s seconds ---\" % round((time.time() - start_time), 4))\n",
    "    printResults(results, top)\n",
    "    print('\\n\\n === word2vec SEARCH ============================================================\\n')\n",
    "    start_time = time.time()\n",
    "    results = word2vec_search(query, removal, method)\n",
    "    print(\" ------------------------------------------------------------- %s seconds ---\" % round((time.time() - start_time), 4))\n",
    "    printResults(results, top)\n",
    "\n",
    "def search_compare(query, top):\n",
    "    search_utility(query, 3, \"SNOWBALL\", top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb5cf82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User given query:\n",
      "\tHow to write a search algorithm?\n",
      "\n",
      "\n",
      " === TF-IDF SEARCH ==============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tsearch algorithm \n",
      "\n",
      "Best match scored cosine similarity of 0.14987489537815366; the worst match scored 0.0.\n",
      "\n",
      " ------------------------------------------------------------- 0.006 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tImplement a basic binary search algorithm\n",
      "code\n",
      "\t\n",
      "def binary_search(arr, x):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] < x:\n",
      "            low = mid + 1\n",
      "        elif arr[mid] > x:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            return mid\n",
      "    return -1\n",
      "        \n",
      "tags\n",
      "\t['python', 'algorithm', 'search', 'binary search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === GloVe SEARCH ===============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tsearch algorithm \n",
      "\n",
      "Best match scored cosine similarity of 0.8485673566135123; the worst match scored 0.009067608624503643.\n",
      "\n",
      " ------------------------------------------------------------- 0.028 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tImplement a basic binary search algorithm\n",
      "code\n",
      "\t\n",
      "def binary_search(arr, x):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] < x:\n",
      "            low = mid + 1\n",
      "        elif arr[mid] > x:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            return mid\n",
      "    return -1\n",
      "        \n",
      "tags\n",
      "\t['python', 'algorithm', 'search', 'binary search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tDijkstra's Algorithm: Find the shortest paths from a start vertex to all other vertices in a weighted graph.\n",
      "code\n",
      "\timport heapq\n",
      "\n",
      "def dijkstra(graph, start):\n",
      "    distances = {vertex: float('infinity') for vertex in graph}\n",
      "    distances[start] = 0\n",
      "    pq = [(0, start)]\n",
      "    while len(pq) > 0:\n",
      "        current_distance, current_vertex = heapq.heappop(pq)\n",
      "        if current_distance > distances[current_vertex]:\n",
      "            continue\n",
      "        for neighbor, weight in graph[current_vertex].items():\n",
      "            distance = current_distance + weight\n",
      "            if distance < distances[neighbor]:\n",
      "                distances[neighbor] = distance\n",
      "                heapq.heappush(pq, (distance, neighbor))\n",
      "    return distances\n",
      "\n",
      "# Example Usage:\n",
      "graph = {\n",
      "    'A': {'B': 1, 'C': 4},\n",
      "    'B': {'A': 1, 'C': 2, 'D': 5},\n",
      "    'C': {'A': 4, 'B': 2, 'D': 1},\n",
      "    'D': {'B': 5, 'C': 1}\n",
      "}\n",
      "distances = dijkstra(graph, 'A')\n",
      "tags\n",
      "\t['algorithm', 'dijkstra', 'graph', 'shortest path']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tFloyd-Warshall Algorithm: Find the shortest paths between all pairs of vertices in a weighted graph.\n",
      "code\n",
      "\tdef floyd_warshall(vertices, edges):\n",
      "    distance = {v: dict.fromkeys(vertices, float('infinity')) for v in vertices}\n",
      "    for v in vertices:\n",
      "        distance[v][v] = 0\n",
      "    for u, v, w in edges:\n",
      "        distance[u][v] = w\n",
      "    for k in vertices:\n",
      "        for i in vertices:\n",
      "            for j in vertices:\n",
      "                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])\n",
      "    return distance\n",
      "tags\n",
      "\t['algorithm', 'floyd-warshall', 'graph', 'shortest path']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === word2vec SEARCH ============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tsearch algorithm \n",
      "\n",
      "Best match scored cosine similarity of 0.8915409989046494; the worst match scored 0.1878863879136198.\n",
      "\n",
      " ------------------------------------------------------------- 0.0315 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tImplement a basic binary search algorithm\n",
      "code\n",
      "\t\n",
      "def binary_search(arr, x):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] < x:\n",
      "            low = mid + 1\n",
      "        elif arr[mid] > x:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            return mid\n",
      "    return -1\n",
      "        \n",
      "tags\n",
      "\t['python', 'algorithm', 'search', 'binary search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tImplement Breadth-First Search (BFS) for a graph\n",
      "code\n",
      "\t\n",
      "def bfs(graph, start):\n",
      "    visited = []\n",
      "    queue = deque([start])\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        if vertex not in visited:\n",
      "            visited.append(vertex)\n",
      "            queue.extend(graph[vertex] - set(visited))\n",
      "    return visited\n",
      "\n",
      "# Example Usage: Use the graph from the DFS example\n",
      "bfs_visited = bfs(graph, 'A')\n",
      "        \n",
      "tags\n",
      "\t['graph', 'bfs']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tDepth-First Search (DFS) on a graph.\n",
      "code\n",
      "\tdef dfs(graph, start, visited=None):\n",
      "    if visited is None:\n",
      "        visited = set()\n",
      "    visited.add(start)\n",
      "    for next in graph[start]:\n",
      "        if next not in visited:\n",
      "            dfs(graph, next, visited)\n",
      "    return visited\n",
      "\n",
      "# Example Usage:\n",
      "graph = {'A': set(['B', 'C']),\n",
      "         'B': set(['A', 'D', 'E']),\n",
      "         'C': set(['A', 'F']),\n",
      "         'D': set(['B']),\n",
      "         'E': set(['B', 'F']),\n",
      "         'F': set(['C', 'E'])}\n",
      "dfs_visited = dfs(graph, 'A')\n",
      "tags\n",
      "\t['algorithm', 'dfs', 'graph']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/f_llb0z97sjdnv60m892yls80000gn/T/ipykernel_5356/603531733.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Test\n",
    "\n",
    "search_compare(\"How to write a search algorithm?\", 3)\n",
    "\n",
    "# results = tfidf_search(\"How to write a search algorithm?\", 3, \"SNOWBALL\")\n",
    "# printResults(results, 5)\n",
    "# print('\\n================================================================================\\n')\n",
    "# results = GloVe_search(\"How to write a search algorithm?\", 3, \"SNOWBALL\")\n",
    "# printResults(results, 5)\n",
    "# print('\\n================================================================================\\n')\n",
    "# results = word2vec_search(\"How to write a search algorithm?\", 3, \"SNOWBALL\")\n",
    "# printResults(results, 5)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "615019cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User given query:\n",
      "\twrite a tree traversal\n",
      "\n",
      "\n",
      " === TF-IDF SEARCH ==============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\ttree travers\n",
      "\n",
      "Best match scored cosine similarity of 0.19982010957894894; the worst match scored 0.0.\n",
      "\n",
      " ------------------------------------------------------------- 0.0053 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tPost-order traversal of a binary tree\n",
      "code\n",
      "\t\n",
      "def post_order_traversal(root):\n",
      "    return post_order_traversal(root.left) + post_order_traversal(root.right) + [root.val] if root else []\n",
      "\n",
      "# Example Usage: Use the binary tree from the in-order traversal example\n",
      "post_order = post_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'post-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tPre-order traversal of a binary tree\n",
      "code\n",
      "\t\n",
      "def pre_order_traversal(root):\n",
      "    return [root.val] + pre_order_traversal(root.left) + pre_order_traversal(root.right) if root else []\n",
      "\n",
      "# Example Usage: Use the binary tree from the in-order traversal example\n",
      "pre_order = pre_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'pre-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tPerform a level-order traversal on a binary tree\n",
      "code\n",
      "\t\n",
      "def level_order_traversal(root):\n",
      "    levels = []\n",
      "    if not root:\n",
      "        return levels\n",
      "    queue = deque([root])\n",
      "    while queue:\n",
      "        level_size = len(queue)\n",
      "        level = []\n",
      "        for i in range(level_size):\n",
      "            node = queue.popleft()\n",
      "            level.append(node.val)\n",
      "            if node.left:\n",
      "                queue.append(node.left)\n",
      "            if node.right:\n",
      "                queue.append(node.right)\n",
      "        levels.append(level)\n",
      "    return levels\n",
      "\n",
      "# Example Usage: Use the binary tree from the previous example\n",
      "level_order = level_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'level-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === GloVe SEARCH ===============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\ttree travers\n",
      "\n",
      "Best match scored cosine similarity of 0.9999999999999999; the worst match scored -0.1419163038672147.\n",
      "\n",
      " ------------------------------------------------------------- 0.0309 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tPost-order traversal of a binary tree\n",
      "code\n",
      "\t\n",
      "def post_order_traversal(root):\n",
      "    return post_order_traversal(root.left) + post_order_traversal(root.right) + [root.val] if root else []\n",
      "\n",
      "# Example Usage: Use the binary tree from the in-order traversal example\n",
      "post_order = post_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'post-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tTraverse a binary tree using in-order traversal\n",
      "code\n",
      "\t\n",
      "class TreeNode:\n",
      "    def __init__(self, val=0, left=None, right=None):\n",
      "        self.val = val\n",
      "        self.left = left\n",
      "        self.right = right\n",
      "\n",
      "def in_order_traversal(root):\n",
      "    return in_order_traversal(root.left) + [root.val] + in_order_traversal(root.right) if root else []\n",
      "\n",
      "# Example Usage:\n",
      "# Construct a binary tree:     1\n",
      "#                            /   \\\n",
      "#                           2     3\n",
      "#                          / \\   /\n",
      "#                         4   5 6\n",
      "root = TreeNode(1, TreeNode(2, TreeNode(4), TreeNode(5)), TreeNode(3, TreeNode(6)))\n",
      "in_order = in_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'in-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tPerform a level-order traversal on a binary tree\n",
      "code\n",
      "\t\n",
      "def level_order_traversal(root):\n",
      "    levels = []\n",
      "    if not root:\n",
      "        return levels\n",
      "    queue = deque([root])\n",
      "    while queue:\n",
      "        level_size = len(queue)\n",
      "        level = []\n",
      "        for i in range(level_size):\n",
      "            node = queue.popleft()\n",
      "            level.append(node.val)\n",
      "            if node.left:\n",
      "                queue.append(node.left)\n",
      "            if node.right:\n",
      "                queue.append(node.right)\n",
      "        levels.append(level)\n",
      "    return levels\n",
      "\n",
      "# Example Usage: Use the binary tree from the previous example\n",
      "level_order = level_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'level-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === word2vec SEARCH ============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\ttree travers\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/f_llb0z97sjdnv60m892yls80000gn/T/ipykernel_5356/603531733.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match scored cosine similarity of 0.9999999999999998; the worst match scored 0.03458672056691623.\n",
      "\n",
      " ------------------------------------------------------------- 0.0303 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tPost-order traversal of a binary tree\n",
      "code\n",
      "\t\n",
      "def post_order_traversal(root):\n",
      "    return post_order_traversal(root.left) + post_order_traversal(root.right) + [root.val] if root else []\n",
      "\n",
      "# Example Usage: Use the binary tree from the in-order traversal example\n",
      "post_order = post_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'post-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tPre-order traversal of a binary tree\n",
      "code\n",
      "\t\n",
      "def pre_order_traversal(root):\n",
      "    return [root.val] + pre_order_traversal(root.left) + pre_order_traversal(root.right) if root else []\n",
      "\n",
      "# Example Usage: Use the binary tree from the in-order traversal example\n",
      "pre_order = pre_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'pre-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tPerform a level-order traversal on a binary tree\n",
      "code\n",
      "\t\n",
      "def level_order_traversal(root):\n",
      "    levels = []\n",
      "    if not root:\n",
      "        return levels\n",
      "    queue = deque([root])\n",
      "    while queue:\n",
      "        level_size = len(queue)\n",
      "        level = []\n",
      "        for i in range(level_size):\n",
      "            node = queue.popleft()\n",
      "            level.append(node.val)\n",
      "            if node.left:\n",
      "                queue.append(node.left)\n",
      "            if node.right:\n",
      "                queue.append(node.right)\n",
      "        levels.append(level)\n",
      "    return levels\n",
      "\n",
      "# Example Usage: Use the binary tree from the previous example\n",
      "level_order = level_order_traversal(root)\n",
      "        \n",
      "tags\n",
      "\t['tree', 'binary tree', 'level-order']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\n",
      "User given query:\n",
      "\tbreadth-first search\n",
      "\n",
      "\n",
      " === TF-IDF SEARCH ==============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadthfirst search\n",
      "\n",
      "Best match scored cosine similarity of 0.23092570590802552; the worst match scored 0.0.\n",
      "\n",
      " ------------------------------------------------------------- 0.0045 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tFind the 'Kth' max value in a binary search tree.\n",
      "code\n",
      "\t\n",
      "class TreeNode:\n",
      "    def __init__(self, val=0, left=None, right=None):\n",
      "        self.val = val\n",
      "        self.left = left\n",
      "        self.right = right\n",
      "\n",
      "def kth_largest(root, k):\n",
      "    # Perform reverse in-order traversal\n",
      "    def reverse_inorder(r):\n",
      "        if r is not None:\n",
      "            yield from reverse_inorder(r.right)\n",
      "            yield r.val\n",
      "            yield from reverse_inorder(r.left)\n",
      "            \n",
      "    gen = reverse_inorder(root)\n",
      "    for _ in range(k):\n",
      "        kth = next(gen)\n",
      "    return kth\n",
      "\n",
      "# Example Usage:\n",
      "# Construct a binary search tree:     5\n",
      "#                                    /   \\\n",
      "#                                   3     7\n",
      "#                                  / \\   / \\\n",
      "#                                 2   4  6   8\n",
      "root = TreeNode(5, TreeNode(3, TreeNode(2), TreeNode(4)), TreeNode(7, TreeNode(6), TreeNode(8)))\n",
      "k = 3\n",
      "kth_largest_value = kth_largest(root, k)  # Returns 6\n",
      "        \n",
      "tags\n",
      "\t['binary search tree', 'kth largest']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === GloVe SEARCH ===============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadthfirst search\n",
      "\n",
      "Best match scored cosine similarity of 0.9070283345744388; the worst match scored -0.12250688312765635.\n",
      "\n",
      " ------------------------------------------------------------- 0.028 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tCheck if a dictionary contains a specific key\n",
      "code\n",
      "\tif 'key_name' in my_dict: print('Key exists')\n",
      "tags\n",
      "\t['python', 'dictionary', 'key']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === word2vec SEARCH ============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadthfirst search\n",
      "\n",
      "Best match scored cosine similarity of 0.8655063215674619; the worst match scored 0.08826960255654713.\n",
      "\n",
      " ------------------------------------------------------------- 0.0276 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tBreadth-First Search (BFS) on a graph.\n",
      "code\n",
      "\tfrom collections import deque\n",
      "\n",
      "def bfs(graph, start):\n",
      "    visited, queue = set(), deque([start])\n",
      "    visited.add(start)\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        for neighbour in graph[vertex]:\n",
      "            if neighbour not in visited:\n",
      "                visited.add(neighbour)\n",
      "                queue.append(neighbour)\n",
      "    return visited\n",
      "\n",
      "# Example Usage:\n",
      "bfs_visited = bfs(graph, 'A')\n",
      "tags\n",
      "\t['algorithm', 'bfs', 'graph']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Test\n",
    "\n",
    "search_compare(\"write a tree traversal\", 3)\n",
    "\n",
    "print('\\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\\n')\n",
    "\n",
    "search_compare(\"breadth-first search\", 3)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59ce6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User given query:\n",
      "\thow to get time in python?\n",
      "\n",
      "\n",
      " === TF-IDF SEARCH ==============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tget time python \n",
      "\n",
      "Best match scored cosine similarity of 0.42800010888082557; the worst match scored 0.0.\n",
      "\n",
      " ------------------------------------------------------------- 0.0044 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tDecorator to time the execution of a function\n",
      "code\n",
      "\t\n",
      "import time\n",
      "\n",
      "def timer(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        start = time.time()\n",
      "        result = func(*args, **kwargs)\n",
      "        end = time.time()\n",
      "        print(f'Function {func.__name__} executed in {end - start:.4f} seconds')\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@timer\n",
      "def example_function():\n",
      "    time.sleep(2)\n",
      "        \n",
      "tags\n",
      "\t['python', 'decorator', 'timer', 'function']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tFunction to add two numbers in Python\n",
      "code\n",
      "\tdef add(a, b): return a + b\n",
      "tags\n",
      "\t['python', 'function', 'math', 'addition']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tReverse a string in Python\n",
      "code\n",
      "\tstring[::-1]\n",
      "tags\n",
      "\t['python', 'string', 'reverse']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === GloVe SEARCH ===============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tget time python \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/f_llb0z97sjdnv60m892yls80000gn/T/ipykernel_5356/603531733.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match scored cosine similarity of 0.8581071615550888; the worst match scored -0.22575424805927483.\n",
      "\n",
      " ------------------------------------------------------------- 0.0315 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tGet the current date and time using datetime\n",
      "code\n",
      "\t\n",
      "from datetime import datetime\n",
      "current_datetime = datetime.now()\n",
      "        \n",
      "tags\n",
      "\t['python', 'datetime', 'current time']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tMerge Sort: Sort an array by dividing it into halves, sorting the halves, and merging them back together.\n",
      "code\n",
      "\tdef merge_sort(arr):\n",
      "    if len(arr) > 1:\n",
      "        mid = len(arr) // 2\n",
      "        L = arr[:mid]\n",
      "        R = arr[mid:]\n",
      "        merge_sort(L)\n",
      "        merge_sort(R)\n",
      "        i = j = k = 0\n",
      "        while i < len(L) and j < len(R):\n",
      "            if L[i] < R[j]:\n",
      "                arr[k] = L[i]\n",
      "                i += 1\n",
      "            else:\n",
      "                arr[k] = R[j]\n",
      "                j += 1\n",
      "            k += 1\n",
      "        while i < len(L):\n",
      "            arr[k] = L[i]\n",
      "            i += 1\n",
      "            k += 1\n",
      "        while j < len(R):\n",
      "            arr[k] = R[j]\n",
      "            j += 1\n",
      "            k += 1\n",
      "tags\n",
      "\t['algorithm', 'merge sort', 'sort']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tCatch exceptions using try-except blocks\n",
      "code\n",
      "\t\n",
      "try:\n",
      "    result = 10 / 0\n",
      "except ZeroDivisionError:\n",
      "    print(\"Cannot divide by zero!\")\n",
      "        \n",
      "tags\n",
      "\t['python', 'exception', 'try-except', 'error handling']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === word2vec SEARCH ============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tget time python \n",
      "\n",
      "Best match scored cosine similarity of 0.7039216924005683; the worst match scored 0.004930049562123867.\n",
      "\n",
      " ------------------------------------------------------------- 0.0295 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tGet the current date and time using datetime\n",
      "code\n",
      "\t\n",
      "from datetime import datetime\n",
      "current_datetime = datetime.now()\n",
      "        \n",
      "tags\n",
      "\t['python', 'datetime', 'current time']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tDecorator to time the execution of a function\n",
      "code\n",
      "\t\n",
      "import time\n",
      "\n",
      "def timer(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        start = time.time()\n",
      "        result = func(*args, **kwargs)\n",
      "        end = time.time()\n",
      "        print(f'Function {func.__name__} executed in {end - start:.4f} seconds')\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@timer\n",
      "def example_function():\n",
      "    time.sleep(2)\n",
      "        \n",
      "tags\n",
      "\t['python', 'decorator', 'timer', 'function']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tWrite a function to get the intersection point of two linked lists.\n",
      "code\n",
      "\t\n",
      "class ListNode:\n",
      "    def __init__(self, x):\n",
      "        self.val = x\n",
      "        self.next = None\n",
      "\n",
      "def get_intersection_node(headA, headB):\n",
      "    nodes_in_B = set()\n",
      "    while headB is not None:\n",
      "        nodes_in_B.add(headB)\n",
      "        headB = headB.next\n",
      "    while headA is not None:\n",
      "        if headA in nodes_in_B:\n",
      "            return headA\n",
      "        headA = headA.next\n",
      "    return None\n",
      "\n",
      "# Example Usage:\n",
      "# Skip the list creation part for brevity. Assume we have two intersecting linked lists.\n",
      "intersection_node = get_intersection_node(list_headA, list_headB)\n",
      "        \n",
      "tags\n",
      "\t['linked list', 'intersection']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Test\n",
    "\n",
    "search_compare(\"how to get time in python?\", 3)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfc1853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/f_llb0z97sjdnv60m892yls80000gn/T/ipykernel_5356/603531733.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim_scores.append(np.dot(query_vector, des_vector) / (np.linalg.norm(query_vector)*np.linalg.norm(des_vector)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User given query:\n",
      "\thow to write breadth first search?\n",
      "\n",
      "\n",
      " === TF-IDF SEARCH ==============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadth first search \n",
      "\n",
      "Best match scored cosine similarity of 0.1690434742544628; the worst match scored 0.0.\n",
      "\n",
      " ------------------------------------------------------------- 0.0057 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tImplement Breadth-First Search (BFS) for a graph\n",
      "code\n",
      "\t\n",
      "def bfs(graph, start):\n",
      "    visited = []\n",
      "    queue = deque([start])\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        if vertex not in visited:\n",
      "            visited.append(vertex)\n",
      "            queue.extend(graph[vertex] - set(visited))\n",
      "    return visited\n",
      "\n",
      "# Example Usage: Use the graph from the DFS example\n",
      "bfs_visited = bfs(graph, 'A')\n",
      "        \n",
      "tags\n",
      "\t['graph', 'bfs']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tPerform breadth-first search (BFS) on a graph\n",
      "code\n",
      "\t\n",
      "from collections import deque\n",
      "\n",
      "def bfs(graph, start):\n",
      "    visited = set()\n",
      "    queue = deque([start])\n",
      "    \n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        if vertex not in visited:\n",
      "            visited.add(vertex)\n",
      "            queue.extend(set(graph[vertex]) - visited)\n",
      "    return visited\n",
      "\n",
      "graph = {1: [2, 3], 2: [4, 5], 3: [5], 4: [], 5: [6], 6: []}\n",
      "bfs_visited = bfs(graph, 1)\n",
      "        \n",
      "tags\n",
      "\t['graph', 'bfs', 'deque']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tBreadth-First Search (BFS) on a graph.\n",
      "code\n",
      "\tfrom collections import deque\n",
      "\n",
      "def bfs(graph, start):\n",
      "    visited, queue = set(), deque([start])\n",
      "    visited.add(start)\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        for neighbour in graph[vertex]:\n",
      "            if neighbour not in visited:\n",
      "                visited.add(neighbour)\n",
      "                queue.append(neighbour)\n",
      "    return visited\n",
      "\n",
      "# Example Usage:\n",
      "bfs_visited = bfs(graph, 'A')\n",
      "tags\n",
      "\t['algorithm', 'bfs', 'graph']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === GloVe SEARCH ===============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadth first search \n",
      "\n",
      "Best match scored cosine similarity of 0.888751997625975; the worst match scored -0.27916773800775374.\n",
      "\n",
      " ------------------------------------------------------------- 0.0291 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tBreadth-First Search (BFS): Traverse a graph level by level, exploring all of the neighbor nodes at the present depth before moving on to nodes at the next depth level.\n",
      "code\n",
      "\tfrom collections import deque\n",
      "\n",
      "def bfs(graph, start):\n",
      "    visited, queue = set(), deque([start])\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        if vertex not in visited:\n",
      "            visited.add(vertex)\n",
      "            queue.extend(graph[vertex] - visited)\n",
      "    return visited\n",
      "tags\n",
      "\t['algorithm', 'bfs', 'graph', 'traversal']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " === word2vec SEARCH ============================================================\n",
      "\n",
      "Searching for processed query of : \n",
      "\tbreadth first search \n",
      "\n",
      "Best match scored cosine similarity of 0.5863626122807525; the worst match scored 0.08014169455444989.\n",
      "\n",
      " ------------------------------------------------------------- 0.0302 seconds ---\n",
      "Result #1:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low, high = 0, len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "    return -1\n",
      "\n",
      "# Example Usage:\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "target = 7\n",
      "index = binary_search(arr, target)  # Returns the index of the target if found, otherwise -1\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #2:\n",
      "\n",
      "description\n",
      "\tBinary Search: Search a sorted array by repeatedly dividing the search interval in half.\n",
      "code\n",
      "\tdef binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        if guess == target:\n",
      "            return mid\n",
      "        if guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "tags\n",
      "\t['algorithm', 'binary search', 'search']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result #3:\n",
      "\n",
      "description\n",
      "\tBreadth-First Search (BFS) on a graph.\n",
      "code\n",
      "\tfrom collections import deque\n",
      "\n",
      "def bfs(graph, start):\n",
      "    visited, queue = set(), deque([start])\n",
      "    visited.add(start)\n",
      "    while queue:\n",
      "        vertex = queue.popleft()\n",
      "        for neighbour in graph[vertex]:\n",
      "            if neighbour not in visited:\n",
      "                visited.add(neighbour)\n",
      "                queue.append(neighbour)\n",
      "    return visited\n",
      "\n",
      "# Example Usage:\n",
      "bfs_visited = bfs(graph, 'A')\n",
      "tags\n",
      "\t['algorithm', 'bfs', 'graph']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Test\n",
    "\n",
    "search_compare(\"how to write breadth first search?\", 3)\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c821820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------------------------------\n",
    "# # EVALUATING:\n",
    "\n",
    "# # --------------------------------------------------------------------------------\n",
    "\n",
    "# def transform_model_data_w_tfidf_vectorizer(preprocessed_text, Y_train,  X_test, Y_test, model_type):\n",
    "#     #vectorize dataset \n",
    "#     tfidf = TfidfVectorizer() \n",
    "#     vectorized_data = tfidf.fit_transform(preprocessed_text)\n",
    "\n",
    "#     #define model\n",
    "#     if model_type == 0:\n",
    "#         model = TfidfVectorizer()\n",
    "#     elif model_type == 1:\n",
    "#         model = MultinomialNB(alpha=0.1)\n",
    "#     else:\n",
    "#         model = BernoulliNB(alpha=0.1)\n",
    "\n",
    "#     model.fit(vectorized_data, Y_train)\n",
    " \n",
    "#     #evaluate model\n",
    "#     predictions = model.predict(tfidf.transform(X_test))\n",
    "\n",
    "#     accuracy = accuracy_score( Y_test, predictions)\n",
    "#     balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "#     precision = precision_score(Y_test, predictions)\n",
    "\n",
    "#     print(\"Accuracy:\",round(100*accuracy,2),'%')\n",
    "#     print(\"Balanced accuracy:\",round(100*balanced_accuracy,2),'%')\n",
    "#     print(\"Precision:\", round(100*precision,2),'%')\n",
    "#     return predictions, model\n",
    "\n",
    "# # --------------------------------------------------------------------------------\n",
    "\n",
    "# # preprocessed_text_1 = [text_clean(text, 'L', True) for text in X_train]\n",
    "# # processed_query = preprocess_text(query, removal, method)\n",
    "\n",
    "# preprocessed_text_1 = [preprocess_text(text, 3, \"SNOWBALL\") for text in X_train]\n",
    "\n",
    "# transform_model_data_w_tfidf_vectorizer(preprocessed_text_1, Y_train,  X_test, Y_test, 0)\n",
    "    \n",
    "# # --------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
